!pip -q install transformers accelerate sentencepiece torch --upgrade import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch_dtype, device_map="auto" if torch.cuda.is_available() else None) generator = pipeline("text-generation", model=model, tokenizer=tokenizer) theme = input("Enter a story theme (e.g., 'space adventure', 'mystery in Chennai'): ").strip() style = input("Optional: style (e.g., 'whimsical', 'noir', 'dramatic') [Enter to skip]: ").strip() or "engaging and vivid" length = input("Optional: target length in words [default 200]: ").strip() try: target_words = max(80, int(length or 200)) except ValueError: target_words = 200 approx_tokens = int(target_words * 1.3) system_msg = ( "You are a skilled fiction writer. Write tightly, avoid clich√©s, use concrete imagery, " "and deliver a clear arc (setup, tension, resolution) with a satisfying ending." ) user_msg = f"Write a short story based on the theme: '{theme}'. Style: {style}. Target length: ~{target_words} words." prompt = f"<|system|>\n{system_msg}\n<|end|>\n<|user|>\n{user_msg}\n<|end|>\n<|assistant|>\n" outputs = generator(prompt, max_new_tokens=approx_tokens, temperature=0.9, top_p=0.95, repetition_penalty=1.1, do_sample=True, pad_token_id=tokenizer.eos_token_id) story = outputs[0]["generated_text"] if "<|assistant|>" in story:
story = story.split("<|assistant|>")[-1] story = story.replace("<|end|>", "").strip() print("\n" + "="*20 + " Your Short Story " + "="*20 + "\n") print(story)
